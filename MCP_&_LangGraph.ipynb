{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Modular AI Network Assistant with MCP & LangGraph\n",
        "\n",
        "\n",
        "This notebook demonstrates the implementation of a scalable AI agent using the **Model Context Protocol (MCP)** standard. By leveraging LangGraph for state orchestration and GlobalPing as a remote MCP server, we create a network diagnostic assistant capable of executing real-time connectivity tests (Ping, Traceroute, DNS).\n",
        "\n",
        "The workflow illustrates how MCP solves the \"tool integration\" problem by decoupling the agent's logic from the tool's implementation. Key technical concepts covered include:\n",
        "\n",
        "* **Multi-Server MCP Client**: Connecting a standard LLM to external, standardized tool servers.\n",
        "* **Asynchronous Execution:** Using Python's asyncio for non-blocking tool usage.\n",
        "* **State Management:** Orchestrating conversation history and tool outputs with LangGraph."
      ],
      "metadata": {
        "id": "mkioRYvdQNDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Depedancies"
      ],
      "metadata": {
        "id": "CAEUL2TPRNeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install langgraph langchain-openai  nest_asyncio gradio\n",
        "# !pip install langchain-mcp-adapters"
      ],
      "metadata": {
        "id": "OhbZoxFz2gZ4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import APIs"
      ],
      "metadata": {
        "id": "CmkfC3nTRQyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive', force_remount = True)\n",
        "\n",
        "\n",
        "# ---------------------------------OpenAI API--------------------------\n",
        "api_key_file = '/content/drive/MyDrive/api_key.txt'\n",
        "with open(api_key_file, 'r', encoding = 'utf-8-sig') as file:\n",
        "    api_key = file.read().strip()\n",
        "\n",
        "print(api_key is not None)\n",
        "os.environ['OPENAI_API_KEY'] = api_key\n",
        "print(\"OpenAI API key loaded:\", os.environ.get('OPENAI_API_KEY') is not None)\n",
        "\n",
        "\n",
        "# --------------------------------GLOBALPING API------------------------\n",
        "globalping_token_file = '/content/drive/MyDrive/globalping.txt'  # Adjust the path if necessary\n",
        "\n",
        "with open(globalping_token_file, 'r') as file:\n",
        "   globalping_token = file.read().strip()\n",
        "\n",
        "os.environ['GLOBALPING_TOKEN'] = globalping_token\n",
        "print(\"GlobalPing Token loaded:\", os.environ.get('GLOBALPING_TOKEN') is not None)"
      ],
      "metadata": {
        "id": "sUNDThAesIHU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a5dddc-3824-4bdd-b28c-d545c6d2ecd4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "True\n",
            "OpenAI API key loaded: True\n",
            "GlobalPing Token loaded: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Network Graph"
      ],
      "metadata": {
        "id": "mi6VKg08RUPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, MessagesState, START, END\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_core.messages import HumanMessage, SystemMessage # Added SystemMessage for robustness\n",
        "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
        "import gradio as gr\n",
        "import os\n",
        "\n",
        "def create_network_graph(tools):\n",
        "    # Model setup: using temperature=0 for deterministic tool usage\n",
        "    llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
        "    llm_with_tools = llm.bind_tools(tools=tools)\n",
        "\n",
        "    async def agent_node(state: MessagesState):\n",
        "        \"\"\"\n",
        "        Process the state and generate a response using the LLM.\n",
        "        We pass the full history (state['messages']) to maintain context.\n",
        "        \"\"\"\n",
        "        return {\"messages\": [await llm_with_tools.ainvoke(state[\"messages\"])]}\n",
        "\n",
        "    tool_node = ToolNode(tools)\n",
        "\n",
        "    builder = StateGraph(MessagesState)\n",
        "\n",
        "    # Define Nodes\n",
        "    builder.add_node('agent', agent_node)\n",
        "    builder.add_node('tools', tool_node)\n",
        "\n",
        "    # Define Edges\n",
        "    builder.add_edge(START, 'agent')\n",
        "    # tools_condition: Built-in logic that routes to 'tools' if the LLM calls one, else END\n",
        "    builder.add_conditional_edges('agent', tools_condition)\n",
        "    builder.add_edge('tools', 'agent')\n",
        "    builder.add_edge('agent', END)\n",
        "\n",
        "    # Persistence\n",
        "    memory = MemorySaver()\n",
        "\n",
        "    return builder.compile(checkpointer=memory)\n",
        "\n",
        "\n",
        "async def main():\n",
        "    print('Initialising MCP Client')\n",
        "\n",
        "    # MCP Client Configuration\n",
        "    # 'Multi' allows connecting to multiple distinct servers (e.g., GlobalPing, Slack, Database) simultaneously.\n",
        "    client = MultiServerMCPClient({\n",
        "        \"globalping\": {\n",
        "            \"transport\": \"http\", # Using SSE (Server-Sent Events) over HTTP\n",
        "            \"url\": \"https://mcp.globalping.dev/mcp\",\n",
        "            \"headers\": {\"Authorization\": f\"Bearer {os.environ['GLOBALPING_TOKEN']}\"}\n",
        "        }\n",
        "    })\n",
        "\n",
        "    print('Fetching tools from MCP Server')\n",
        "    # Dynamic Tool Loading: No manual wrappers needed. The client fetches definitions from the server.\n",
        "    tools = await client.get_tools()\n",
        "\n",
        "    # Optional: Inspect loaded tools\n",
        "    for tool in tools:\n",
        "        print(f'Tool Name: {tool.name}')\n",
        "        print(f'Tool Description: {tool.description[:100]}...')\n",
        "        print('_' * 30)\n",
        "\n",
        "    print('Create Network Graph')\n",
        "    graph = create_network_graph(tools)\n",
        "\n",
        "    async def interact(message, history):\n",
        "        config = {'configurable': {'thread_id': 'session1'}}\n",
        "\n",
        "        # We inject a SystemMessage to guide the model on parameter formatting (avoids API 400 errors)\n",
        "        inputs = {'messages': [\n",
        "            SystemMessage(content=\"You are a network assistant. When using Globalping tools, ensure the 'target' parameter is a simple string (e.g., 'google.com') and not an object.\"),\n",
        "            HumanMessage(content=message)\n",
        "        ]}\n",
        "\n",
        "        final_state = await graph.ainvoke(inputs, config)\n",
        "        return final_state['messages'][-1].content\n",
        "\n",
        "\n",
        "    print('Launching Gradio...')\n",
        "    demo = gr.ChatInterface(\n",
        "        fn=interact,\n",
        "        type='messages',\n",
        "        title='Network Assistant with Globalping (MCP)'\n",
        "    )\n",
        "    demo.queue()\n",
        "    demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "bAatuNJ_3Bgi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "await main()"
      ],
      "metadata": {
        "id": "uLmFxMFwFg08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "96a1e36e-4cbc-4727-c1f5-4602b69113a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialising MCP Client\n",
            "Fetching tools from MCP Server\n",
            "Tool Name: ping\n",
            "Tool Description: Measure network latency, packet loss, and reachability to a target (domain or IP) from globally dist...\n",
            "______________________________\n",
            "Tool Name: traceroute\n",
            "Tool Description: Trace the network path to a target (domain or IP) from global locations. Use this tool to identify w...\n",
            "______________________________\n",
            "Tool Name: dns\n",
            "Tool Description: Resolve DNS records (A, AAAA, MX, etc.) for a domain from global locations. Use this tool to verify ...\n",
            "______________________________\n",
            "Tool Name: mtr\n",
            "Tool Description: Run an MTR (My Traceroute) diagnostic, which combines Ping and Traceroute. Use this tool to analyze ...\n",
            "______________________________\n",
            "Tool Name: http\n",
            "Tool Description: Send HTTP/HTTPS requests (GET, HEAD or OPTIONS) to a URL from global locations. Use this tool to che...\n",
            "______________________________\n",
            "Tool Name: locations\n",
            "Tool Description: Retrieve the list of available Globalping probe locations. Use this tool to find specific countries,...\n",
            "______________________________\n",
            "Tool Name: limits\n",
            "Tool Description: Check current API rate limits and remaining credits. Use this tool to monitor your usage quota and v...\n",
            "______________________________\n",
            "Tool Name: getMeasurement\n",
            "Tool Description: Retrieve the full details of a past measurement using its ID. Use this tool to access raw JSON data,...\n",
            "______________________________\n",
            "Tool Name: compareLocations\n",
            "Tool Description: Get a guide on how to run comparison tests using the exact same probes as a previous measurement. Us...\n",
            "______________________________\n",
            "Tool Name: help\n",
            "Tool Description: Get a comprehensive guide to the Globalping MCP server. Use this tool to learn about available tools...\n",
            "______________________________\n",
            "Tool Name: authStatus\n",
            "Tool Description: Check the current authentication status. Use this tool to verify if the user is logged in and has a ...\n",
            "______________________________\n",
            "Tool Name: get_more_tools\n",
            "Tool Description: Check for additional tools whenever your task might benefit from specialized capabilities - even if ...\n",
            "______________________________\n",
            "Create Network Graph\n",
            "Launching Gradio...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://e8e76c9b9f24ad6d74.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e8e76c9b9f24ad6d74.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WR5WTPD2Pkhf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}